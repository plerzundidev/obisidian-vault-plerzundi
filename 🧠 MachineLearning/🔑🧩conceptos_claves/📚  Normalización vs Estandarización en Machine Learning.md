## Tabla de Contenidos
1. [Introducci√≥n](#introducci√≥n)  
2. [¬øQu√© es Normalizaci√≥n?](#qu√©-es-normalizaci√≥n)  
3. [¬øQu√© es Estandarizaci√≥n?](#qu√©-es-estandarizaci√≥n)  
4. [Comparaci√≥n Detallada](#comparaci√≥n-detallada)  
5. [¬øCu√°ndo Usar Cada T√©cnica?](#cu√°ndo-usar-cada-t√©cnica)  
6. [Implementaci√≥n Pr√°ctica](#implementaci√≥n-pr√°ctica)  
7. [Casos de Uso por Dominio](#casos-de-uso-por-dominio)  
8. [Errores Comunes y Buenas Pr√°cticas](#errores-comunes-y-buenas-pr√°cticas)  
9. [Preguntas Frecuentes](#preguntas-frecuentes)  

---

## Introducci√≥n
<a name="introducci√≥n"></a>
La **normalizaci√≥n** y **estandarizaci√≥n** son t√©cnicas fundamentales para escalar datos en Machine Learning. Su objetivo es garantizar que las variables contribuyan equitativamente al modelo, evitando sesgos por diferencias de escala.

---

## ¬øQu√© es Normalizaci√≥n? <a name="qu√©-es-normalizaci√≥n"></a>
### Definici√≥n
Transforma los datos para que est√©n en un **rango espec√≠fico** (t√≠picamente [0, 1]).  
**F√≥rmula**:  
$$[
X_{\text{norm}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
]$$

### Caracter√≠sticas Clave
- **Rango**: [0, 1] o personalizado (ej: [-1, 1]).  
- **Efecto en Distribuci√≥n**: Mantiene la forma original.  
- **Outliers**: Altamente sensible. Un solo outlier extremo distorsiona el escalado.  

### Ejemplo Visual

Datos Originales: [10, 20, 30, 40, 50]  
Normalizados: [0.0, 0.25, 0.5, 0.75, 1.0]


---

## ¬øQu√© es Estandarizaci√≥n? <a name="qu√©-es-estandarizaci√≥n"></a>
### Definici√≥n
Ajusta los datos para tener **media = 0** y **desviaci√≥n est√°ndar = 1**.  
**F√≥rmula**:  
$$[
X_{\text{std}} = \frac{X - \mu}{\sigma}
]$$

### Caracter√≠sticas Clave
- **Distribuci√≥n**: Asume forma gaussiana (no obligatoria, pero ideal).  
- **Outliers**: M√°s robusta que la normalizaci√≥n.  
- **Interpretaci√≥n**: Valores negativos/positivos indican desviaciones respecto a la media.  

### Ejemplo Visual

Datos Originales: [10, 20, 30, 40, 50]  
Estandarizados: [-1.41, -0.71, 0.0, 0.71, 1.41]


---

## Comparaci√≥n Detallada <a name="comparaci√≥n-detallada"></a>
| **Aspecto**               | **Normalizaci√≥n (Min-Max)**       | **Estandarizaci√≥n (Z-Score)**      |
|---------------------------|------------------------------------|-------------------------------------|
| **Rango**                 | [0, 1] o definido                 | Sin l√≠mites                         |
| **Media (Œº)**             | No controlada                     | 0                                   |
| **Desviaci√≥n Est√°ndar (œÉ)**| Variable                          | 1                                   |
| **Outliers**              | Alta sensibilidad                 | Robustez moderada                   |
| **Algoritmos Recomendados**| KNN, Redes Neuronales, K-Means    | SVM, Regresi√≥n Lineal, PCA          |

---

## ¬øCu√°ndo Usar Cada T√©cnica? <a name="cu√°ndo-usar-cada-t√©cnica"></a>
### Normalizaci√≥n ‚úÖ
- **Algoritmos basados en distancias**:  
  - K-Nearest Neighbors (KNN): Las distancias euclidianas son sensibles a la escala.  
  - K-Means: Evita que clusters sean dominados por variables de mayor magnitud.  
- **Redes Neuronales**: Acelera la convergencia en optimizaci√≥n (ej.: usar activaciones como Sigmoid).  
- **Procesamiento de im√°genes**: Escalar p√≠xeles a [0, 1] o [-1, 1].  

### Estandarizaci√≥n ‚úÖ
- **Algoritmos basados en gradientes**:  
  - Regresi√≥n Lineal/Log√≠stica: Mejora la interpretaci√≥n de coeficientes.  
  - SVM: Optimiza el margen de separaci√≥n entre clases.  
- **T√©cnicas estad√≠sticas**: PCA (maximiza la varianza en componentes principales).  
- **Datos con distribuci√≥n normal**: Aprovecha propiedades estad√≠sticas.  

---

## Implementaci√≥n Pr√°ctica <a name="implementaci√≥n-pr√°ctica"></a>
### C√≥digo en Python (scikit-learn)

```python

from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Normalizaci√≥n
minmax_scaler = MinMaxScaler(feature_range=(0, 1))
X_normalized = minmax_scaler.fit_transform(X_train)

# Estandarizaci√≥n
std_scaler = StandardScaler()
X_standardized = std_scaler.fit_transform(X_train)

# ¬°Importante! Aplica misma transformaci√≥n a datos de prueba:
X_test_normalized = minmax_scaler.transform(X_test)
X_test_standardized = std_scaler.transform(X_test)
```


## Buenas Pr√°cticas en el Escalado de Datos <a name="errores-comunes-y-buenas-pr√°cticas"></a>

### 1. Dividir Datos Antes de Escalar
- **Problema**: Si escalas antes de dividir en entrenamiento y prueba, introduces *data leakage* (filtraci√≥n de informaci√≥n).  
- **Soluci√≥n**:  
  1. Divide el dataset en `train` y `test`.  
  2. Ajusta el scaler **solo con los datos de entrenamiento**.  
  3. Aplica la misma transformaci√≥n a los datos de prueba.  

**C√≥digo Ejemplo**:
```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Ajustar scaler SOLO en train
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Aplicar misma transformaci√≥n a test
X_test_scaled = scaler.transform(X_test)
```

### 2. Manejo de Outliers

- **Normalizaci√≥n (Min-Max)**:
    
    - **Riesgo**: Un solo outlier extremo comprime el rango del resto de los datos.
    - **Soluci√≥n**:
        
        - Eliminar outliers antes de escalar.
            
        - Usar t√©cnicas como **winsorization** (limitar valores extremos).   
- **Estandarizaci√≥n (Z-Score)**:
    - **Ventaja**: Menos sensible a outliers que Min-Max.
    - **Alternativa**: Usar `RobustScaler` (basado en medianas y rangos intercuart√≠licos).


**C√≥digo Ejemplo con RobustScaler**:	

```python
from sklearn.preprocessing import RobustScaler

robust_scaler = RobustScaler()
X_robust = robust_scaler.fit_transform(X_train)
```


### 3. Guardar Par√°metros del Scaler para Producci√≥n

- **Problema**: En producci√≥n, debes aplicar la misma transformaci√≥n que usaste en entrenamiento.
- **Soluci√≥n**: Serializar el scaler (ej.: con `pickle` o `joblib`).

**C√≥digo Ejemplo**:

```python
import joblib

# Guardar scaler
joblib.dump(scaler, 'scaler.pkl')

# Cargar scaler en producci√≥n
loaded_scaler = joblib.load('scaler.pkl')
X_new_scaled = loaded_scaler.transform(X_new)
```

### 4. Visualizar el Efecto del Escalado

- **Herramientas**:
    - Histogramas y boxplots para comparar distribuciones antes/despu√©s.
    - Gr√°ficos de densidad para ver cambios en la forma.

**C√≥digo Ejemplo con Matplotlib**:

```python
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Antes del escalado
axes[0].hist(X_train[:, 0], bins=30, color='blue', alpha=0.7)
axes[0].set_title('Datos Originales')

# Despu√©s del escalado
axes[1].hist(X_train_scaled[:, 0], bins=30, color='red', alpha=0.7)
axes[1].set_title('Datos Estandarizados')

plt.show()
```

### 5. No Escalar Variables Categ√≥ricas o Binarias

- **Variables Categ√≥ricas**:
    - Ejemplo: G√©nero ("Hombre", "Mujer").
    - **Soluci√≥n**: Usar `OneHotEncoder` o `OrdinalEncoder`.
- **Variables Binarias (0/1)**:
    - No requieren escalado (ya est√°n en un rango acotado).

**C√≥digo Ejemplo**:

```python
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder()
X_categorical_encoded = encoder.fit_transform(X_categorical)
```


### 6. Usar Pipelines para Automatizar

- **Ventaja**: Evita errores manuales y asegura consistencia.
- **Herramienta**: `sklearn.pipeline.Pipeline`.


**C√≥digo Ejemplo**:

```python

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# Crear pipeline: escalado + modelo
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])

# Entrenar y predecir
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
```

### 7. Validar con M√©tricas Clave

- **M√©tricas a Monitorear**:
    - **Precisi√≥n**: ¬øMejora el modelo despu√©s de escalar?
    - **Tiempo de Convergencia**: ¬øEl modelo tarda menos en entrenar?
        
- **T√©cnica**: Usar validaci√≥n cruzada para comparar resultados.

**Ejemplo con Cross-Validation**:

```python

from sklearn.model_selection import cross_val_score

# Sin escalar
scores_raw = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)

# Con escalado
pipeline = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression())])
scores_scaled = cross_val_score(pipeline, X_train, y_train, cv=5)

print(f"Sin escalar: {scores_raw.mean():.2f} | Con escalado: {scores_scaled.mean():.2f}")
```

### 8. Considerar el Contexto del Dominio <a name="casos-de-uso-por-dominio"></a>

La elecci√≥n entre normalizaci√≥n y estandarizaci√≥n debe alinearse con las convenciones y necesidades espec√≠ficas de cada industria. Aqu√≠ m√°s ejemplos:

---

#### Ejemplo 1: **Bioinform√°tica** üß¨
- **Datos**: Expresi√≥n g√©nica (ej.: valores de 0 a 10,000 en microarrays).  
- **Problema**: Genes con rangos de expresi√≥n muy dispares (ej.: un gen var√≠a entre 0-100 y otro entre 0-10,000).  
- **T√©cnica Recomendada**: **Normalizaci√≥n (Min-Max)**.  
- **Raz√≥n**:  
  - Permite comparar genes en una escala com√∫n (ej.: [0, 1]).  
  - Evita que genes altamente expresivos dominen an√°lisis de clustering.  

---

#### Ejemplo 2: **Energ√≠a Renovable** ‚òÄÔ∏è
- **Datos**: Radiaci√≥n solar (0‚Äì1000 W/m¬≤) y velocidad del viento (0‚Äì20 m/s).  
- **Problema**: Modelar la relaci√≥n entre variables para predecir producci√≥n energ√©tica.  
- **T√©cnica Recomendada**: **Estandarizaci√≥n**.  
- **Raz√≥n**:  
  - Si se usa una regresi√≥n lineal, los coeficientes reflejar√°n mejor el impacto relativo de cada variable.  
  - La radiaci√≥n solar tiene mayor magnitud, pero no necesariamente mayor importancia.  

---

#### Ejemplo 3: **Marketing Digital** üì±
- **Datos**:  
  - Tiempo en p√°gina (segundos).  
  - N√∫mero de clics (entero no negativo).  
- **Problema**: Segmentar usuarios con K-Means para campa√±as personalizadas.  
- **T√©cnica Recomendada**: **Normalizaci√≥n**.  
- **Raz√≥n**:  
  - Sin escalado, "tiempo en p√°gina" (ej.: 0-600 segundos) dominar√≠a sobre "clics" (0-10).  
  - Equilibra la contribuci√≥n de ambas variables en el c√°lculo de distancias.  

---

#### Ejemplo 4: **Agricultura de Precisi√≥n** üåæ
- **Datos**:  
  - Humedad del suelo (0‚Äì100%).  
  - Concentraci√≥n de nitr√≥geno (0‚Äì500 ppm).  
- **Problema**: Predecir rendimiento de cultivos con una red neuronal.  
- **T√©cnica Recomendada**: **Normalizaci√≥n (a [-1, 1])**.  
- **Raz√≥n**:  
  - Las activaciones como *tanh* funcionan mejor con entradas en rangos sim√©tricos.  
  - Acelera la convergencia del gradiente descendente.  

---

#### Ejemplo 5: **Finanzas Cuantitativas** üíπ
- **Datos**:  
  - Retornos diarios de acciones (-5% a +5%).  
  - Volumen de transacciones (0‚Äì10 millones de USD).  
- **Problema**: Detecci√≥n de anomal√≠as con Autoencoders.  
- **T√©cnica Recomendada**: **Estandarizaci√≥n**.  
- **Raz√≥n**:  
  - El volumen tiene una distribuci√≥n con colas pesadas (outliers comunes).  
  - La estandarizaci√≥n hace que los errores de reconstrucci√≥n sean comparables entre variables.  

---

#### Ejemplo 6: **Audio Processing** üéµ
- **Datos**: Amplitud de onda (valores entre -1 y 1).  
- **Problema**: Entrenar un modelo para generar m√∫sica con GANs.  
- **T√©cnica Recomendada**: **No escalar** (o normalizar a [-1, 1]).  
- **Raz√≥n**:  
  - Los datos ya est√°n en un rango acotado por naturaleza.  
  - Las capas de normalizaci√≥n dentro de la GAN (ej.: BatchNorm) manejan el escalado interno.  

---

#### Ejemplo 7: **Log√≠stica** üöö
- **Datos**:  
  - Tiempo de entrega (horas).  
  - Distancia recorrida (kil√≥metros).  
- **Problema**: Optimizar rutas con un modelo de regresi√≥n.  
- **T√©cnica Recomendada**: **Estandarizaci√≥n**.  
- **Raz√≥n**:  
  - Si la distancia tiene una distribuci√≥n sesgada (ej.: mayor√≠a de pedidos locales, pocos internacionales), la estandarizaci√≥n reduce el sesgo en los coeficientes.  

---

#### Ejemplo 8: **Videojuegos** üéÆ
- **Datos**:  
  - Puntuaci√≥n del jugador (0‚Äì1000 puntos).  
  - Tiempo de juego diario (0‚Äì8 horas).  
- **Problema**: Sistema de recomendaci√≥n de desaf√≠os con SVM.  
- **T√©cnica Recomendada**: **Estandarizaci√≥n**.  
- **Raz√≥n**:  
  - SVM utiliza m√°rgenes de separaci√≥n basados en distancias.  
  - La puntuaci√≥n podr√≠a tener outliers (ej.: jugadores profesionales con 5000 puntos).  

---

#### Ejemplo 9: **Salud Mental** üß†
- **Datos**:  
  - Escala de ansiedad (1‚Äì10, ordinal).  
  - N√∫mero de sesiones terap√©uticas (0‚Äì20).  
- **Problema**: Clasificar riesgo de depresi√≥n con Regresi√≥n Log√≠stica.  
- **T√©cnica Recomendada**: **Estandarizaci√≥n**.  
- **Raz√≥n**:  
  - La regresi√≥n log√≠stica usa descenso de gradiente, que converge m√°s r√°pido con datos estandarizados.  
  - La escala ordinal (1‚Äì10) no es lineal en su impacto (mejor tratarla como num√©rica continua).  

---

#### Ejemplo 10: **Space Exploration** üöÄ
- **Datos**:  
  - Temperatura exterior (-150¬∞C a +150¬∞C).  
  - Presi√≥n atmosf√©rica (0‚Äì1000 hPa).  
- **Problema**: Predecir fallos en sensores con Random Forest.  
- **T√©cnica Recomendada**: **No escalar**.  
- **Raz√≥n**:  
  - Los modelos basados en √°rboles (como Random Forest) no requieren escalado.  
  - Ahorra tiempo de preprocesamiento sin afectar el rendimiento.  