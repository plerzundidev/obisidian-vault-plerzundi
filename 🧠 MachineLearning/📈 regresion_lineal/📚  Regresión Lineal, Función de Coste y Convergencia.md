

## 1. Conceptos Fundamentales

### 1.1 **Regresi√≥n Lineal**

Es un modelo que busca encontrar una **recta** (o hiper plano en dimensiones superiores) que mejor se ajuste a los datos. La f√≥rmula general es:

$$y = a + b \cdot x$$

- **`a`**: Intercepto (valor de `y` cuando `x = 0`).
- **`b`**: Pendiente (cambio en `y` por unidad de `x`).

Para m√∫ltiples variables, la f√≥rmula se expande a:

$$y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n$$

Que puede escribirse de forma matricial como:

$$y = \theta^T X$$

---

### 1.2 **Funci√≥n de Coste (MSE - Error Cuadr√°tico Medio)**

Mide el error promedio entre las predicciones del modelo (`≈∑`) y los valores reales (`y`).

#### **Versi√≥n 1: Con `1/(2m)`**

$$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (≈∑_i - y_i)^2$$

- **Derivadas**:

$$\frac{\partial J}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (≈∑_i - y_i) \cdot x_{ij}$$

(Donde $x_{i0} = 1$ para el t√©rmino independiente)

#### **Versi√≥n 2: Con `1/m`**

$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (≈∑_i - y_i)^2$$

- **Derivadas**:

$$\frac{\partial J}{\partial \theta_j} = \frac{2}{m} \sum_{i=1}^{m} (≈∑_i - y_i) \cdot x_{ij}$$

> **Nota**: El factor 1/2 en la primera versi√≥n se usa para simplificar la derivada, eliminando el factor 2 que surge al derivar el t√©rmino cuadr√°tico.

---

### 1.3 **Convergencia con Gradiente Descendiente**

Proceso iterativo para ajustar los par√°metros $\theta$ minimizando el coste:

#### **Actualizaci√≥n de Par√°metros**:

$$\theta_j^{\text{nuevo}} = \theta_j^{\text{antiguo}} - \alpha \cdot \frac{\partial J}{\partial \theta_j}$$

- **`Œ±`**: Tasa de aprendizaje.

**Visualizaci√≥n del Gradiente Descendiente:**

- La funci√≥n de coste forma una superficie en el espacio
- Cada punto representa una combinaci√≥n de par√°metros
- El algoritmo "desciende" hacia el punto m√°s bajo (m√≠nimo)

#### **Tipos de Gradiente Descendiente**:

1. **Batch Gradient Descent**: Usa todos los datos para cada actualizaci√≥n
2. **Stochastic Gradient Descent (SGD)**: Usa un solo dato para cada actualizaci√≥n
3. **Mini-Batch Gradient Descent**: Usa un subconjunto de datos para cada actualizaci√≥n

---

## 2. Caso de Negocio: Predecir Ventas de Helados üç¶

### **2.1 Datos Hist√≥ricos**

|Temperatura (¬∞C)|Ventas (helados)|
|---|---|
|20|50|
|25|80|
|30|120|
|15|40|
|35|150|

---

### **2.2 Definir el Modelo Inicial**

Valores iniciales:

$$a = 10, \quad b = 3$$

Predicciones iniciales:

- Para 20¬∞C: $≈∑ = 10 + 3 \cdot 20 = 70$ (Error: 70 - 50 = 20)
- Para 25¬∞C: $≈∑ = 10 + 3 \cdot 25 = 85$ (Error: 85 - 80 = 5)
- Para 30¬∞C: $≈∑ = 10 + 3 \cdot 30 = 100$ (Error: 100 - 120 = -20)
- Para 15¬∞C: $≈∑ = 10 + 3 \cdot 15 = 55$ (Error: 55 - 40 = 15)
- Para 35¬∞C: $≈∑ = 10 + 3 \cdot 35 = 115$ (Error: 115 - 150 = -35)

---

### **2.3 Calcular el Coste (Versi√≥n `1/(2m)`)**

$$J(10, 3) = \frac{1}{2 \cdot 5} \left[(70-50)^2 + (85-80)^2 + (100-120)^2 + (55-40)^2 + (115-150)^2 \right]$$ $$= \frac{1}{10} \left[400 + 25 + 400 + 225 + 1225 \right]$$ $$= \frac{2275}{10} = 227.5$$

---

### **2.4 Calcular Derivadas Parciales (Versi√≥n `1/(2m)`)**

- **Derivada respecto a `a`**:

$$\frac{\partial J}{\partial a} = \frac{1}{5} \left[(70-50) + (85-80) + (100-120) + (55-40) + (115-150) \right]$$ $$= \frac{1}{5} \left[20 + 5 + (-20) + 15 + (-35) \right]$$ $$= \frac{1}{5} \cdot (-15) = -3$$

- **Derivada respecto a `b`** (corregida):

$$\frac{\partial J}{\partial b} = \frac{1}{5} \left[(70-50) \cdot 20 + (85-80) \cdot 25 + (100-120) \cdot 30 + (55-40) \cdot 15 + (115-150) \cdot 35 \right]$$ $$= \frac{1}{5} \left[20 \cdot 20 + 5 \cdot 25 + (-20) \cdot 30 + 15 \cdot 15 + (-35) \cdot 35 \right]$$ $$= \frac{1}{5} \left[400 + 125 - 600 + 225 - 1225 \right]$$ $$= \frac{1}{5} \cdot (-1075) = -215$$

---

### **2.5 Ajustar Par√°metros**

Con tasa de aprendizaje `Œ± = 0.01`:

$$a_{\text{nuevo}} = 10 - 0.01 \cdot (-3) = 10 + 0.03 = 10.03$$  
$$b_{\text{nuevo}} = 3 - 0.01 \cdot (-215) = 3 + 2.15 = 5.15$$

---

### **2.6 Segunda Iteraci√≥n (Demostrativa)**

Con los nuevos par√°metros $a = 10.03, b = 5.15$:

Predicciones actualizadas:

- Para 20¬∞C: $≈∑ = 10.03 + 5.15 \cdot 20 = 113.03$ (Error: 113.03 - 50 = 63.03)
- Para 25¬∞C: $≈∑ = 10.03 + 5.15 \cdot 25 = 138.78$ (Error: 138.78 - 80 = 58.78)
- Para 30¬∞C: $≈∑ = 10.03 + 5.15 \cdot 30 = 164.53$ (Error: 164.53 - 120 = 44.53)
- Para 15¬∞C: $≈∑ = 10.03 + 5.15 \cdot 15 = 87.28$ (Error: 87.28 - 40 = 47.28)
- Para 35¬∞C: $≈∑ = 10.03 + 5.15 \cdot 35 = 190.28$ (Error: 190.28 - 150 = 40.28)

Coste actualizado:

$$J(10.03, 5.15) = \frac{1}{10} \left[63.03^2 + 58.78^2 + 44.53^2 + 47.28^2 + 40.28^2 \right]$$ $$= \frac{1}{10} \cdot 13662.39 = 1366.24$$

> Observaci√≥n: El coste aument√≥ de 227.5 a 1366.24. Esto significa que necesitamos ajustar la tasa de aprendizaje, ya que es demasiado grande y nos est√° haciendo "saltar" demasiado lejos.

---

### **2.7 Convergencia con Tasa de Aprendizaje Ajustada**

Con una tasa de aprendizaje menor, por ejemplo `Œ± = 0.001`:

$$a_{\text{nuevo}} = 10 - 0.001 \cdot (-3) = 10.003$$  
$$b_{\text{nuevo}} = 3 - 0.001 \cdot (-215) = 3.215$$

Despu√©s de m√∫ltiples iteraciones (alrededor de 10,000 con `Œ± = 0.001`), los par√°metros convergen aproximadamente a:

$$a \approx -12.2, \quad b \approx 4.7$$

**Modelo Final**:

$$≈∑ = -12.2 + 4.7 \cdot x$$

- **Predicci√≥n para 28¬∞C**:

$$≈∑ = -12.2 + 4.7 \cdot 28 \approx 119.4 \text{ helados}$$

---

## 3. Implementaci√≥n en Python

```python
import numpy as np
import matplotlib.pyplot as plt

# Datos de ejemplo
X = np.array([20, 25, 30, 15, 35])  # Temperatura
y = np.array([50, 80, 120, 40, 150])  # Ventas

# Par√°metros iniciales
a = 10
b = 3
alpha = 0.0001  # Tasa de aprendizaje
iterations = 50000

# Historiales para visualizaci√≥n
cost_history = []
a_history = []
b_history = []

# Gradiente descendiente
for i in range(iterations):
    # C√°lculo de predicciones
    y_pred = a + b * X
    
    # C√°lculo del coste
    cost = (1/(2*len(X))) * np.sum((y_pred - y)**2)
    cost_history.append(cost)
    
    # C√°lculo de derivadas
    da = (1/len(X)) * np.sum(y_pred - y)
    db = (1/len(X)) * np.sum((y_pred - y) * X)
    
    # Actualizaci√≥n de par√°metros
    a = a - alpha * da
    b = b - alpha * db
    
    # Guardar historiales
    a_history.append(a)
    b_history.append(b)

print(f"Par√°metros finales: a = {a:.2f}, b = {b:.2f}")
print(f"Coste final: {cost_history[-1]:.2f}")
```

---

## 4. Visualizaci√≥n del Proceso de Convergencia

### 4.1 **Evoluci√≥n del Coste**

La evoluci√≥n del coste t√≠picamente muestra:

- Disminuci√≥n r√°pida al principio
- Desaceleraci√≥n gradual
- Estabilizaci√≥n cuando se acerca al m√≠nimo

### 4.2 **Evoluci√≥n de los Par√°metros**

Los par√°metros suelen:

- Cambiar r√°pidamente al principio
- Luego ajustarse m√°s finamente
- Eventualmente estabilizarse cuando el modelo converge

### 4.3 **Ajuste del Modelo Final**

Un buen ajuste del modelo final deber√≠a:

- Pasar cerca de la mayor√≠a de los puntos
- No estar sesgado hacia ninguna regi√≥n espec√≠fica de los datos
- Capturar la tendencia general sin sobreajustarse a puntos espec√≠ficos

---

## 5. Regresi√≥n Lineal M√∫ltiple: Ampliando el Modelo

La regresi√≥n lineal m√∫ltiple extiende el concepto a m√∫ltiples variables predictoras:

$$y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n$$

### 5.1 **Ejemplo: Predecir Precios de Casas**

Variables predictoras:

- Tama√±o (m¬≤)
- N√∫mero de habitaciones
- Antig√ºedad (a√±os)

|Tama√±o|Habitaciones|Antig√ºedad|Precio (miles $)|
|---|---|---|---|
|100|2|15|150|
|120|3|10|180|
|150|3|5|220|
|80|1|20|120|
|200|4|2|300|

El modelo ser√°:

$$\text{Precio} = \theta_0 + \theta_1 \cdot \text{Tama√±o} + \theta_2 \cdot \text{Habitaciones} + \theta_3 \cdot \text{Antig√ºedad}$$

### 5.2 **Implementaci√≥n del Ejemplo en Python**

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Datos
data = {
    'Tama√±o': [100, 120, 150, 80, 200],
    'Habitaciones': [2, 3, 3, 1, 4],
    'Antig√ºedad': [15, 10, 5, 20, 2],
    'Precio': [150, 180, 220, 120, 300]
}

df = pd.DataFrame(data)

# Caracter√≠sticas y target
X = df[['Tama√±o', 'Habitaciones', 'Antig√ºedad']]
y = df['Precio']

# Dividir en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crear y entrenar el modelo
model = LinearRegression()
model.fit(X_train, y_train)

# Coeficientes y t√©rmino independiente
print(f"T√©rmino independiente (Œ∏‚ÇÄ): {model.intercept_:.2f}")
print("Coeficientes:")
for feature, coef in zip(X.columns, model.coef_):
    print(f"  {feature}: {coef:.2f}")

# Predicciones
y_pred = model.predict(X_test)

# Evaluaci√≥n
print(f"Error cuadr√°tico medio: {mean_squared_error(y_test, y_pred):.2f}")
print(f"Coeficiente de determinaci√≥n (R¬≤): {r2_score(y_test, y_pred):.2f}")
```

---

## 6. Problemas Comunes y Soluciones

### 6.1 **Overfitting (Sobreajuste)**

Cuando el modelo se ajusta demasiado a los datos de entrenamiento y no generaliza bien.

**Soluciones:**

- Regularizaci√≥n (Ridge, Lasso)
- M√°s datos de entrenamiento
- Reducir complejidad del modelo

### 6.2 **Underfitting (Subajuste)**

Cuando el modelo es demasiado simple y no captura la estructura de los datos.

**Soluciones:**

- Aumentar complejidad del modelo
- Incluir m√°s caracter√≠sticas
- Feature engineering

### 6.3 **Alta Multicolinealidad**

Cuando hay alta correlaci√≥n entre caracter√≠sticas.

**Soluciones:**

- An√°lisis de componentes principales (PCA)
- Eliminar caracter√≠sticas redundantes
- Regularizaci√≥n Ridge

### 6.4 **Escala de Caracter√≠sticas**

Cuando las caracter√≠sticas tienen diferentes escalas.

**Soluciones:**

- Normalizaci√≥n (Min-Max)
- Estandarizaci√≥n (Z-score)

---

## 7. Regularizaci√≥n para Regresi√≥n Lineal

### 7.1 **Ridge Regression (L2)**

A√±ade penalizaci√≥n por la suma de cuadrados de coeficientes:

$$J(\theta) = MSE + \lambda \sum_{j=1}^{n} \theta_j^2$$

### 7.2 **Lasso Regression (L1)**

A√±ade penalizaci√≥n por la suma de valores absolutos de coeficientes:

$$J(\theta) = MSE + \lambda \sum_{j=1}^{n} |\theta_j|$$

### 7.3 **Implementaci√≥n de Ridge en Python**

```python
from sklearn.linear_model import Ridge

# Crear y entrenar el modelo con regularizaci√≥n Ridge
ridge_model = Ridge(alpha=1.0)  # alpha es el par√°metro de regularizaci√≥n Œª
ridge_model.fit(X_train, y_train)

# Coeficientes
print("Coeficientes con Ridge:")
for feature, coef in zip(X.columns, ridge_model.coef_):
    print(f"  {feature}: {coef:.2f}")
```

---

## 8. Interpretaci√≥n de Resultados

### 8.1 **Coeficientes**

Cada coeficiente representa el cambio esperado en la variable objetivo por unidad de cambio en la caracter√≠stica correspondiente, manteniendo constantes las dem√°s caracter√≠sticas.

### 8.2 **Intercepto**

Representa el valor esperado de la variable objetivo cuando todas las caracter√≠sticas son cero.

### 8.3 **R¬≤ (Coeficiente de Determinaci√≥n)**

Mide la proporci√≥n de la varianza total de la variable objetivo que es explicada por el modelo.

- **R¬≤ = 1**: El modelo explica perfectamente la variaci√≥n.
- **R¬≤ = 0**: El modelo no explica nada de la variaci√≥n.
- **R¬≤ < 0**: El modelo es peor que simplemente usar la media.

---

## 9. Evaluaci√≥n y Validaci√≥n del Modelo

### 9.1 **T√©cnicas de Validaci√≥n**

- Validaci√≥n cruzada (k-folds)
- Train/Test Split
- Leave-One-Out Cross-Validation
- Validaci√≥n cronol√≥gica (para series temporales)

### 9.2 **M√©tricas de Evaluaci√≥n**

- Error Cuadr√°tico Medio (MSE)
- Ra√≠z del Error Cuadr√°tico Medio (RMSE)
- Error Absoluto Medio (MAE)
- Coeficiente de Determinaci√≥n (R¬≤)

---

## 10. Resumen de F√≥rmulas

| **Concepto**         | **F√≥rmula (`1/(2m)`)**                  | **F√≥rmula (`1/m`)**                     |
| -------------------- | --------------------------------------- | --------------------------------------- |
| **Funci√≥n de Coste** | $$J = \frac{1}{2m} \sum (≈∑_i - y_i)^2$$ | $$J = \frac{1}{m} \sum (≈∑_i - y_i)^2$$  |
| **Derivada de `Œ∏j`** | $$\frac{1}{m} \sum (≈∑_i - y_i) x_{ij}$$ | $$\frac{2}{m} \sum (≈∑_i - y_i) x_{ij}$$ |
| **Ridge (L2)**       | $$J + \lambda \sum \theta_j^2$$         | $$J + \lambda \sum \theta_j^2$$         |
| **Lasso (L1)**       | $$J + \lambda \sum$$                    | $$\theta_j$$                            |

---

## 11. Consideraciones Pr√°cticas

1. **Preparaci√≥n de datos**:
    
    - Manejo de valores faltantes
    - Detecci√≥n y tratamiento de outliers
    - Codificaci√≥n de variables categ√≥ricas
2. **Selecci√≥n de caracter√≠sticas**:
    
    - Feature importance
    - Forward/Backward selection
    - T√©cnicas de reducci√≥n de dimensionalidad
3. **Hiperpar√°metros importantes**:
    
    - Tasa de aprendizaje (Œ±)
    - Par√°metro de regularizaci√≥n (Œª)
    - N√∫mero de iteraciones
4. **Herramientas y librer√≠as**:
    
    - Scikit-learn (Python)
    - StatsModels (Python)
    - R (lm, glmnet)
    - TensorFlow y PyTorch para modelos m√°s complejos

---

## 12. Notas Finales

1. **Coherencia en las f√≥rmulas**:
    
    - `1/(2m)` siempre est√° en la **funci√≥n de coste**, y las derivadas usan `1/m`.
    - `1/m` en la funci√≥n de coste implica derivadas con `2/m`.
2. **Tasa de aprendizaje (`Œ±`)**:
    
    - Si usas `1/m`, reduce `Œ±` a la mitad para compensar el factor `2` en las derivadas.
    - Una tasa de aprendizaje demasiado grande puede causar divergencia
    - Una tasa demasiado peque√±a puede hacer que la convergencia sea muy lenta
    -
3. **Regresi√≥n lineal vs. otros algoritmos**:
    
    - Ventajas: Interpretabilidad, rapidez, base para modelos complejos
    - Desventajas: Asume relaci√≥n lineal, sensible a outliers

---

## 13. Ejemplo Gr√°fico: Convergencia del Modelo de Helados

### Modelo Inicial vs. Final

Imaginemos una visualizaci√≥n de nuestro modelo de ventas de helados:

**Modelo Inicial (a=10, b=3)**:

- Predicciones alejadas de los valores reales
- Error total alto (MSE = 227.5)

**Modelo Final (a=-12.2, b=4.7)**:

- Predicciones muy cercanas a los valores reales
- Error total m√≠nimo
- Captura bien la relaci√≥n temperatura-ventas

### Proceso de Convergencia

Durante el entrenamiento observar√≠amos:

1. El modelo inicial empieza lejos de los datos
2. Con cada iteraci√≥n, la l√≠nea se acerca a los puntos
3. La pendiente aumenta desde 3 hasta 4.7
4. El intercepto disminuye desde 10 hasta -12.2
5. El error (MSE) disminuye progresivamente
6. Eventualmente el modelo "converge" y se estabiliza

---

## 14. Aplicaciones Pr√°cticas en Machine Learning

### Predicci√≥n de Ventas

- Predecir ventas basadas en gastos de publicidad
- Variables: presupuesto en TV, radio, peri√≥dicos, etc.
- Objetivo: optimizar distribuci√≥n de presupuesto

### Predicci√≥n de Precios

- Predecir precios de viviendas, acciones, productos
- Variables: caracter√≠sticas, hist√≥ricos, tendencias
- Objetivo: decisiones de inversi√≥n o fijaci√≥n de precios

### An√°lisis de Influencia

- Determinar qu√© variables impactan m√°s un resultado
- Variables: cualquier factor medible
- Objetivo: toma de decisiones estrat√©gicas
